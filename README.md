**End to End Fabric Data Factory Project**

The objective of this project is to design and implement a scalable, cloud-based data engineering pipeline for entertainment platform (Netflix data) using Microsoft Fabric services. The pipelines will enable efficient data ingestion, transformation by integrating data from Source (One Lake) to Data destination (Lakehouse), ensuring data quality by performing efficient data pipeline orchestration strategies with CI/CD.

![Architecture Diagram](https://github.com/NisanthTumu/FabricDataFactory/blob/main/FabricDataFactory.png)

**âœ… Key Goals:**
â€¢	Automate Data Ingestion: Seamlessly ingest data from diverse sources (One Lake, ADLSGen2) into Lakehouse using Fabric Data Factory.
â€¢	Implementing activities: such as if condition, get metadata, foreach, set variable and email notifications.
â€¢	Enable Data Transformation & Enrichment: Leverage Fabric Dataflows Gen2 for data processing with power query as low-no code manner and using diagram view.
â€¢	Creating pyspark notebooks for final serving transformations and using Invoke pipeline activity to run all the child pipelines.
â€¢	Performing CI/CD: This involves effortless deployment all the resources and files to another workspace environment.
â€¢	Ensure Production-Readiness: Apply best practices for performance optimization, error handling, monitoring, and maintenance.

**ðŸŽ¯ Learnings:**

â€¢ Setting up Fabric Data Factory from scratch â€¢ Configuring ADLS Gen2 storage â€¢ Understanding Medallion Architecture  â€¢ Implementing data ingestion from HTTP endpoints (GitHub) and ADLSGen2 with copy activity â€¢ Creating your first data pipeline â€¢ Real-world implementation scenarios â€¢ If Condition Activityâ€¢ Data Factory Real Time Scenario â€¢ Load Data from Azure Data Lake â€¢ Get Metadata Activityâ€¢ ForEach Activity for Parametrized Pipelines â€¢ Set Variable Activity â€¢ Pipeline Monitoring and Email Notifications â€¢ Data Flow Gen2â€¢ ETL Pipelines using Data Flows â€¢ PySpark Tutorial â€¢ CI/CD 

**ðŸ’¡ Technical Skills Covered:**

â€¢	Fabric Data Factory configuration 
â€¢	Data Lake storage setup 
â€¢	Basic data ingestion patterns 
â€¢	Raw data handling strategies 
â€¢	Data pipeline orchestration 
â€¢	Error handling & monitoring
â€¢	ETL pipelines using Dataflows Gen2
â€¢	PySpark tutorial with notebooks
â€¢	Performing CI/CD

**ðŸ’¡ Technical Deep Dive:**

â€¢	Advanced data transformation patterns 
â€¢	Data enrichment strategies 
â€¢	End-to-end pipeline testing 
â€¢	Monitoring and maintenance

